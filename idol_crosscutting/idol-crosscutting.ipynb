{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on python3.8\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import argparse\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "import random\n",
    "import dlib\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "from moviepy.editor import *\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법1: Random distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDistance:\n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        dur_end = min(reference_clip.duration, compare_clip.duration)\n",
    "        return random.randrange(1,100), min(dur_end, random.randrange(3,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법2: Face distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDistance:\n",
    "    def __init__(self, shape_predictor_path, face_embedding_penalty=None):\n",
    "        self.skip_frame_rate = 4                                  #(1)\n",
    "        self.minimax_frames = 5                                   #(2)\n",
    "        self.shape_predictor = shape_predictor_path               #(3)\n",
    "        self.face_embedding_penalty = face_embedding_penalty      #(4)\n",
    "\n",
    "        \n",
    "    def extract_landmark(self, reference_clip, compare_clip):\n",
    "        # (1) 영상 저장 및 face landmark detect model 불러오기\n",
    "        self.clips =[reference_clip, compare_clip]                # (1)-a\n",
    "        detector = dlib.get_frontal_face_detector()               # (1)-b\n",
    "        predictor = dlib.shape_predictor(self.shape_predictor)    # (1)-c\n",
    "        clips_frame_info = []\n",
    "        for clip in self.clips:\n",
    "            # (2) 각 영상의 정보를 저장하기 위해 loop마다 초기화하기\n",
    "            i=0                                                   # (2)-a\n",
    "            every_frame_info= []\t\t\t                      # (2)-b\n",
    "                    \n",
    "            while True:\n",
    "                # (3) 각 영상에서 face Landmark 얻기\n",
    "                frame = clip.get_frame(i*1.0/clip.fps)            # (3)-a\n",
    "                i+=self.skip_frame_rate                           # (3)-b\n",
    "                if (i*1.0/clip.fps)> clip.duration:               # (3)-c\n",
    "                    break\n",
    "                \n",
    "                frame = imutils.resize(frame, width=800)          # (3)-d\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)    # (3)-e\n",
    "                rects = detector(gray, 0)                         # (3)-f\n",
    "                # (4) 얻은 face Landmark를 가공해서 every_frame_info에 저장하기\n",
    "                if len(rects)>0:                                  # (4)-a\n",
    "                    max_width = 0\n",
    "                    max_rect = None                               # (4)-b\n",
    "                    for rect in rects:                            # (4)-c\n",
    "                        if int(rects[0].width()) > max_width:\n",
    "                            max_rect = rect\n",
    "                    shape = predictor(gray, max_rect)             # (4)-d\n",
    "                    shape = face_utils.shape_to_np(shape)         # (4)-e\n",
    "                    every_frame_info.append(shape)                # (4)-f\n",
    "                else:\n",
    "                    every_frame_info.append([])\n",
    "            # (5) 영상 Frame별 landmark 정보 clips_frame_info에 저장하기\n",
    "            clips_frame_info.append(np.array(every_frame_info))   # (5)-a\n",
    "        cv2.destroyAllWindows()\n",
    "        return clips_frame_info\n",
    "\n",
    "\n",
    "    \n",
    "    def embedding_cosine_distance(self, reference_frame, compare_frame):\n",
    "        face_detector = MTCNN(select_largest=True)\n",
    "        embed_model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "        \n",
    "        reference_frame = np.array(reference_frame)\n",
    "        compare_frame = np.array(compare_frame)\n",
    "        try:\n",
    "            reference_frame_detected = face_detector(reference_frame)\n",
    "            compare_frame_detected = face_detector(compare_frame)\n",
    "        except:\n",
    "            cosine_dist = 1\n",
    "            return cosine_dist\n",
    "        \n",
    "        reference_frame_embed = embed_model(reference_frame_detected.unsqueeze(0)).detach().numpy()\n",
    "        compare_frame_embed = embed_model(compare_frame_detected.unsqueeze(0)).detach().numpy()\n",
    "        reference_frame_embed = np.squeeze(reference_frame_embed)\n",
    "        compare_frame_embed = np.squeeze(compare_frame_embed)\n",
    "        cosine_dist = 1 - np.dot(reference_frame_embed, compare_frame_embed) / (np.linalg.norm(reference_frame_embed) * np.linalg.norm(compare_frame_embed))\n",
    "        return cosine_dist\n",
    "\n",
    "\n",
    "    def get_all_frame_distance(self, clips_frame_info, min_size):\n",
    "        dist_arr = []                                             # (1)\n",
    "        for i in range(min_size-1):                               # (2)\n",
    "            # (3)\n",
    "            if len(clips_frame_info[0][i])>0 and len(clips_frame_info[1][i+1])>0:\n",
    "            # (4)\n",
    "                l = 36\n",
    "                r = 45\n",
    "                left_eye = ((clips_frame_info[0][i][l][0] - clips_frame_info[1][i+1][l][0])**2 + \n",
    "                    (clips_frame_info[0][i][l][1] - clips_frame_info[1][i+1][l][1])**2)**0.5\n",
    "                right_eye = ((clips_frame_info[0][i][r][0] - clips_frame_info[1][i+1][r][0])**2 + \n",
    "                    (clips_frame_info[0][i][r][1] - clips_frame_info[1][i+1][r][1])**2)**0.5\n",
    "                total_diff = left_eye + right_eye\n",
    "                dist_arr.append(total_diff)                        # (5)\n",
    "            else:  \n",
    "                dist_arr.append(None)\n",
    "        return dist_arr\n",
    "\n",
    "\n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        # (1) 거리 계산에 필요한 정보들 먼저 얻기\n",
    "        # (1)-a\n",
    "        clips_frame_info = self.extract_landmark(reference_clip, compare_clip)\n",
    "        # (1)-b\n",
    "        min_size = min(len(clips_frame_info[0]),len(clips_frame_info[1]))\n",
    "        # (1)-c \n",
    "        dist_arr = self.get_all_frame_distance(clips_frame_info, min_size)\n",
    "        # (1)-d\n",
    "        clips =[reference_clip,compare_clip]\n",
    "        minimax_frames = self.minimax_frames\n",
    "        min_diff = np.float('Inf')\n",
    "        min_idx = 0\n",
    "        # (2) 최소 거리가 되는 영상과 시간 찾기\n",
    "        # (2)-a\n",
    "        for i in range(min_size - (minimax_frames - 1)):\n",
    "            # (2)-b\n",
    "            start_minmax_idx = 0 if (i - minimax_frames)<0 else i - minimax_frames\n",
    "            # (2)-c\n",
    "            if (None not in dist_arr[start_minmax_idx :i + minimax_frames]):\n",
    "                # (2)-d\n",
    "                tmp_max = np.max(dist_arr[start_minmax_idx:i + minimax_frames])\n",
    "                if min_diff > tmp_max:\n",
    "                    min_diff = tmp_max\n",
    "                    min_idx = i\n",
    "        \n",
    "        # (3) Face Embedding Penalty 추가하기\n",
    "        if self.face_embedding_penalty != None and min_diff < np.float(\"Inf\"):\n",
    "            ref_frame = reference_clip.get_frame(min_idx * 1.0/reference_clip.fps)\n",
    "            frame = compare_clip.get_frame(min_idx * 1.0/compare_clip.fps)\n",
    "            cosine_dist = self.embedding_cosine_distance(ref_frame, frame)\n",
    "            min_diff += cosine_dist * self.face_embedding_penalty\n",
    "        # (4) 두 영상 간의 최소 거리 정보 리턴\n",
    "        return min_diff, (min_idx*self.skip_frame_rate)/self.clips[0].fps\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법3: Pose distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDistance:\n",
    "    def __init__(self):\n",
    "        self.SKIP_FRAME_RATE = 10                          # (1)\n",
    "        self.MINIMAX_FRAME = 4                             # (2)\n",
    "        self.model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True) # (3)\n",
    "        self.model.eval()\n",
    "        os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\n",
    "    def extract_boxes(self, reference_clip, compare_clip):\n",
    "        # (1) 변수 초기화\n",
    "        self.clips = [reference_clip, compare_clip]\n",
    "        clips_frame_info = []\n",
    "        for clip in self.clips:\n",
    "        # (2) 각 영상의 정보를 저장하기 위해 loop마다 초기화하기\n",
    "            i = 0\n",
    "            every_frame_info = []\n",
    "            while True:\n",
    "            # (3) Faster R-CNN을 이용해 물체 판별하기\n",
    "                i+=self.SKIP_FRAME_RATE                     # (3)-a\n",
    "                if (i*1.0/clip.fps)> clip.duration:         # (3)-b\n",
    "                    break\n",
    "                            \n",
    "                frame = clip.get_frame(i*1.0/clip.fps)      # (3)-c\n",
    "                frame = imutils.resize(frame, width=640)    # (3)-d\n",
    "                frame = frame/255                           # (3)-e\n",
    "                frame = np.transpose(frame, (2,0,1))        # (3)-f\n",
    "                x = [torch.from_numpy(frame).float()]       # (3)-g\n",
    "                predictions = self.model(x)                 # (3)-h\n",
    "                prediction= predictions[0]\n",
    "                # (4) 판별정보 재가공하기\n",
    "                # (4)-a\n",
    "                each_box_list = zip(prediction['boxes'].tolist(), prediction['labels'].tolist(), prediction['scores'].tolist())\n",
    "                # (4)-b\n",
    "                filtered_box_list = filter(lambda x: x[1]==1 and x[2] >= 0.95, each_box_list)\n",
    "                # (4)-c\n",
    "                filtered_center_dot_list = list(map(lambda x: [(x[0][0]+x[0][2])/2, (x[0][1]+x[0][3])/2], filtered_box_list))\n",
    "                # (4)-d\n",
    "                sorted_dot_list = sorted(filtered_center_dot_list, key = lambda x: x[0])\n",
    "                # (5) 재가공한 정보 every_frame_info에 저장하기\n",
    "                every_frame_info.append(sorted_dot_list)\n",
    "            # (6) 영상 Frame별 landmark 정보 clips_frame_info에 저장하기\n",
    "            clips_frame_info.append(np.array(every_frame_info))\n",
    "        return clips_frame_info\n",
    "\n",
    "\n",
    "    def get_all_frame_distance(self, clips_frame_info, min_size):\n",
    "        dist_arr = list()\n",
    "        for i in range(min_size):\n",
    "            # (1)\n",
    "            if len(clips_frame_info[0][i])>0 and len(clips_frame_info[1][i])>0:\n",
    "                # (2)\n",
    "                ref_frame_dots = clips_frame_info[0][i]\n",
    "                compare_frame_dots = clips_frame_info[1][i]\n",
    "                # (3)\n",
    "                min_dot_num = min(len(ref_frame_dots), len(compare_frame_dots))\n",
    "                # (4)\n",
    "                dot_num_diff = abs(len(ref_frame_dots)- len(compare_frame_dots))\n",
    "                # (5)\n",
    "                penalty = ((self.clips[0].w **2 + self.clips[0].h**2)**0.5) * abs(len(ref_frame_dots)-len(compare_frame_dots)) \n",
    "                # (6)\n",
    "                total_diff = penalty * dot_num_diff\n",
    "                # (7)\n",
    "                for dot_idx in range(min_dot_num):\n",
    "                    total_diff += ((ref_frame_dots[dot_idx][0] - compare_frame_dots[dot_idx][0])**2 + (ref_frame_dots[dot_idx][1] - compare_frame_dots[dot_idx][1])**2)**0.5\n",
    "                # (8)\n",
    "                dist_arr.append(total_diff)\n",
    "            else:\n",
    "                dist_arr.append(None)\n",
    "        return dist_arr\n",
    "\n",
    "\n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        # (1) 거리 계산에 필요한 정보들 먼저 얻기\n",
    "        # (1)-a\n",
    "        clips_frame_info = self.extract_boxes(reference_clip, compare_clip)\n",
    "        # (1)-b\n",
    "        min_size = min(len(clips_frame_info[0]),len(clips_frame_info[1]))\n",
    "        # (1)-c\n",
    "        dist_arr = self.get_all_frame_distance(clips_frame_info, min_size)  \n",
    "        # (1)-d\n",
    "        min_diff = np.float('Inf')\n",
    "        min_idx = 0 \n",
    "        # (2) 최소 거리가 되는 영상과 시간 찾기\n",
    "        # (2)-a\n",
    "        for i in range(min_size-(self.MINIMAX_FRAME-1)):\n",
    "            # (2)-b\n",
    "            start_minmax_idx = 0 if (i - self.MINIMAX_FRAME)<0 else i - self.MINIMAX_FRAME\n",
    "            # (2)-c\n",
    "            if (None not in dist_arr[start_minmax_idx :i + self.MINIMAX_FRAME]):\n",
    "            # (2)-d\n",
    "                tmp_max = np.max(dist_arr[i:i+self.MINIMAX_FRAME])\n",
    "                if min_diff > tmp_max:\n",
    "                    min_diff = tmp_max\n",
    "                    min_idx = i\n",
    "        # (3) 두 영상 간의 최소 거리 정보 리턴\n",
    "        return min_diff, (min_idx*self.SKIP_FRAME_RATE)/reference_clip.fps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crosscut:\n",
    "    def __init__(self, dist_obj, video_path, output_path):\n",
    "        self.videos_path = video_path \t\t# (1)\n",
    "        self.output_path = output_path \t\t# (2)\n",
    "        self.min_time = 1000.0 \t\t\t\t# (3)\n",
    "        video_num = len(os.listdir(self.videos_path)) \t # (4)\n",
    "        self.start_times = [0] * video_num \t\t         # (5)\n",
    "        self.window_time = 10 \t\t\t\t# (6)\n",
    "        self.padded_time = 4 \t\t\t\t# (7)\n",
    "        self.dist_obj = dist_obj \t\t    # (8)\n",
    "        self.audioclip = None \t\t\t\t# (9)\n",
    "        self.extracted_clips_array = [] \t# (10)\n",
    "        self.con_clips = [] \t\t\t\t# (11)\n",
    "\n",
    "    \n",
    "    def video_alignment(self):\n",
    "        for i in range(len(os.listdir(self.videos_path))):\n",
    "            video_path = os.path.join(self.videos_path, sorted(os.listdir(self.videos_path))[i])\n",
    "            clip = VideoFileClip(video_path)\n",
    "            clip = clip.subclip(self.start_times[i], clip.duration)\n",
    "            if self.min_time > clip.duration:\n",
    "                self.audioclip = clip.audio\n",
    "                self.min_time = clip.duration\n",
    "            self.extracted_clips_array.append(clip)\n",
    "        print('LOGGER-- {} Video Will Be Mixed'.format(len(self.extracted_clips_array)))\n",
    "\n",
    "\n",
    "    \n",
    "    def select_next_clip(self, t, current_idx):\n",
    "        # (1) 거리 측정에 필요한 변수 초기화하기\n",
    "        cur_t = t\n",
    "        next_t = min(t+self.window_time, self.min_time)\n",
    "        \n",
    "        reference_clip = self.extracted_clips_array[current_idx].subclip(cur_t, next_t)\n",
    "        d = float(\"Inf\")\n",
    "        cur_clip = None\n",
    "        min_idx = (current_idx+1)%len(self.extracted_clips_array)\n",
    "\n",
    "        # (2) 비교 영상들과 현재 영상의 거리 측정하기\n",
    "        for video_idx in range(len(self.extracted_clips_array)):\n",
    "            if video_idx == current_idx:\n",
    "                continue\n",
    "            clip = self.extracted_clips_array[video_idx].subclip(cur_t, next_t) \n",
    "            cur_d, plus_frame = self.dist_obj.distance(reference_clip, clip) \n",
    "            print(current_idx, video_idx, cur_d, cur_t + plus_frame)\n",
    "            if d > cur_d:\n",
    "                d = cur_d\n",
    "                min_idx = video_idx\n",
    "                next_t = cur_t + plus_frame\n",
    "                cur_clip = reference_clip.subclip(0, plus_frame)\n",
    "        \n",
    "        # (3) 다음 교차편집 지점 전까지 현재 영상 저장하기\n",
    "        if cur_clip: \n",
    "            clip = cur_clip \n",
    "        else:\n",
    "            clip = reference_clip\n",
    "        self.con_clips.append(clip)\n",
    "        \n",
    "        # (4) 현재 시간을 갱신하고 다음에 사용할 영상 인덱스 반환하기\n",
    "        t = next_t\n",
    "        return t, min_idx\n",
    "\n",
    "\n",
    "        \n",
    "    def add_padding(self, t, next_idx):\n",
    "        print(\"idx : {}\".format(next_idx))\n",
    "        pad_clip = self.extracted_clips_array[next_idx].subclip(t, min(self.min_time,t+self.padded_time))\n",
    "        self.con_clips.append(pad_clip)\n",
    "        \n",
    "        t = min(self.min_time,t + self.padded_time)\n",
    "        return t, next_idx\n",
    "\n",
    "        \n",
    "    def write_video(self):\n",
    "        final_clip = concatenate_videoclips(self.con_clips)\n",
    "        if self.audioclip != None:\n",
    "            print(\"Not None\")\n",
    "            final_clip.audio = self.audioclip\n",
    "        final_clip.write_videofile(self.output_path)\n",
    "        return final_clip    \n",
    "\n",
    "    \n",
    "    def generate_video(self):\n",
    "        self.video_alignment()\n",
    "        t = 3\n",
    "        current_idx = 0\n",
    "        self.con_clips.append(self.extracted_clips_array[current_idx].subclip(0, min(t, int(self.min_time))))\n",
    "        while t < int(self.min_time):\n",
    "            t, min_idx = self.select_next_clip(t, current_idx)\n",
    "            t, current_idx = self.add_padding(t, min_idx)\n",
    "        final_clip = self.write_video()\n",
    "        return final_clip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "method = 'random' \t\t\t\t\t\t\t                    #  (1)\n",
    "video_path = 'fifth_season' \t\t\t\t\t\t            #  (2)\n",
    "output_path = 'my_stagemix.mp4' \t\t\t\t\t\t        #  (3)\n",
    "shape_predictor_path = 'shape_predictor_68_face_landmarks.dat' \t#  (4)\n",
    "face_embedding_penalty = 100 # or None \t\t\t\t\t        #  (5)\n",
    "\n",
    "# (6)\n",
    "print(output_path)\n",
    "if method == 'random':\n",
    "    random_distance = RandomDistance()\n",
    "    cross_cut = Crosscut(random_distance, video_path, output_path)\n",
    "elif method == 'face':\n",
    "    face_distance = FaceDistance(shape_predictor_path, face_embedding_penalty)\n",
    "    cross_cut = Crosscut(face_distance, video_path, output_path)\n",
    "elif method == 'pose':\n",
    "    pose_distance = PoseDistance()\n",
    "    cross_cut = Crosscut(pose_distance, video_path, output_path)\n",
    "cross_cut.generate_video()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
